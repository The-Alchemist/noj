[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Noj Documentation",
    "section": "",
    "text": "1 Preface\nNoj (scinojure) is an opinionated way to use the emerging Clojure data stack.\nIt collects a few of the main dependencies together with functions allowing to conveniently use them together.\nSource:\nArtifact:\nStatus: Some parts of the underlying libraries are stable. Some part of Noj are still experimental, and the API will change. These details should be clarified soon.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#existing-chapters-in-this-book",
    "href": "index.html#existing-chapters-in-this-book",
    "title": "Noj Documentation",
    "section": "1.1 Existing chapters in this book:",
    "text": "1.1 Existing chapters in this book:\n\nDatasets\nPython\nStatistics\nVisualization\nStatistical Visualization\nMore visualization examples\nMachine learning specific functionality in tech.ml.dataset\nMachine learning\nAutoML using metamorph pipelines\nOrdinary least squares with interactions\nCurve fitting example\n\n\nsource: notebooks/index.clj",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "2  Datasets",
    "section": "",
    "text": "(ns datasets\n  (:require [tablecloth.api :as tc]\n            [scicloj.kindly.v4.kind :as kind]))\n\nFor our tutorials here, let us fetch some datasets from Rdatasets\n\n(def iris\n  (-&gt; \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\"\n      (tc/dataset {:key-fn keyword})\n      (tc/rename-columns {:Sepal.Length :sepal-length\n                          :Sepal.Width :sepal-width\n                          :Petal.Length :petal-length\n                          :Petal.Width :petal-width\n                          :Species :species})))\n\n\niris\n\nhttps://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv [150 6]:\n\n\n\n\n\n\n\n\n\n\n\n:rownames\n:sepal-length\n:sepal-width\n:petal-length\n:petal-width\n:species\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n…\n…\n…\n…\n…\n…\n\n\n140\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n141\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n142\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n143\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n144\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n145\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n146\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n147\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n148\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n149\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n150\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n(def mtcars\n  (-&gt; \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\"\n      (tc/dataset {:key-fn keyword})))\n\n\nmtcars\n\nhttps://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv [32 12]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:rownames\n:mpg\n:cyl\n:disp\n:hp\n:drat\n:wt\n:qsec\n:vs\n:am\n:gear\n:carb\n\n\n\n\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\nDuster 360\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\nMerc 230\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\nMerc 280\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nDodge Challenger\n15.5\n8\n318.0\n150\n2.76\n3.520\n16.87\n0\n0\n3\n2\n\n\nAMC Javelin\n15.2\n8\n304.0\n150\n3.15\n3.435\n17.30\n0\n0\n3\n2\n\n\nCamaro Z28\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\nPontiac Firebird\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\nFiat X1-9\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\nPorsche 914-2\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\nLotus Europa\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\nFord Pantera L\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\nFerrari Dino\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\nMaserati Bora\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\nVolvo 142E\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\nsource: notebooks/datasets.clj",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "3  Python",
    "section": "",
    "text": "3.1 Using Python visualizations\nNoj offers methods to include Python plots in Kindly visualizations: the vis.python/with-pyplot macro and the vis.python/pyplot function.\nThey are based on the Parens for Pyplot blog post at Squid’s blog.\nhttps://seaborn.pydata.org/tutorial/introduction",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "python.html#using-python-visualizations",
    "href": "python.html#using-python-visualizations",
    "title": "3  Python",
    "section": "",
    "text": "(require-python '[numpy :as np]\n                '[numpy.random :as np.random]\n                'matplotlib.pyplot\n                '[seaborn :as sns]\n                'json)\n\n\n:ok\n\n\n(def sine-data\n  (-&gt; {:x (range 0 (* 3 np/pi) 0.1)}\n      tc/dataset\n      (tc/add-column :y #(fun/sin (:x %)))))\n\n\n(vis.python/with-pyplot\n  (matplotlib.pyplot/plot\n   (:x sine-data)\n   (:y sine-data)))\n\n\n\n(vis.python/pyplot\n #(matplotlib.pyplot/plot\n   (:x sine-data)\n   (:y sine-data)))\n\n\n\n\n(let [tips (sns/load_dataset \"tips\")]\n  (sns/set_theme)\n  (vis.python/pyplot\n   #(sns/relplot :data tips\n                 :x \"total_bill\"\n                 :y \"tip\"\n                 :col \"time\"\n                 :hue \"smoker\"\n                 :style \"smoker\"\n                 :size \"size\")))\n\n\n\n:bye\n\n\n:bye\n\n\nsource: notebooks/python.clj",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python</span>"
    ]
  },
  {
    "objectID": "stats.html",
    "href": "stats.html",
    "title": "4  Statistics",
    "section": "",
    "text": "4.1 Example data",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "stats.html#example-data",
    "href": "stats.html#example-data",
    "title": "4  Statistics",
    "section": "",
    "text": "(def iris\n  (-&gt; \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\"\n      (tc/dataset {:key-fn keyword})\n      (tc/rename-columns {:Sepal.Length :sepal-length\n                          :Sepal.Width :sepal-width\n                          :Petal.Length :petal-length\n                          :Petal.Width :petal-width\n                          :Species :species})))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "stats.html#correlation-matrices",
    "href": "stats.html#correlation-matrices",
    "title": "4  Statistics",
    "section": "4.2 Correlation matrices",
    "text": "4.2 Correlation matrices\nThe stats/calc-correlations-matrix function commputes the correlation matrix of selected columns of a given dataset, organizing the resulting data as a dataset.\n\n(-&gt; iris\n    (stats/calc-correlations-matrix\n     [:sepal-length :sepal-width :petal-length :petal-width]))\n\n_unnamed [16 3]:\n\n\n\n:col-1\n:col-2\n:corr\n\n\n\n\n:sepal-length\n:sepal-length\n1.00000000\n\n\n:sepal-length\n:sepal-width\n-0.11000000\n\n\n:sepal-length\n:petal-length\n0.87000000\n\n\n:sepal-length\n:petal-width\n0.81000000\n\n\n:sepal-width\n:sepal-length\n-0.11000000\n\n\n:sepal-width\n:sepal-width\n1.00000000\n\n\n:sepal-width\n:petal-length\n-0.41999999\n\n\n:sepal-width\n:petal-width\n-0.36000001\n\n\n:petal-length\n:sepal-length\n0.87000000\n\n\n:petal-length\n:sepal-width\n-0.41999999\n\n\n:petal-length\n:petal-length\n1.00000000\n\n\n:petal-length\n:petal-width\n0.95999998\n\n\n:petal-width\n:sepal-length\n0.81000000\n\n\n:petal-width\n:sepal-width\n-0.36000001\n\n\n:petal-width\n:petal-length\n0.95999998\n\n\n:petal-width\n:petal-width\n1.00000000",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "stats.html#multivariate-regression",
    "href": "stats.html#multivariate-regression",
    "title": "4  Statistics",
    "section": "4.3 Multivariate regression",
    "text": "4.3 Multivariate regression\nThe stats/regression-model function computes a regressiom model (using scicloj.ml) and adds some relevant information such as the R^2 measure.\n\n(-&gt; iris\n    (stats/regression-model\n     :sepal-length\n     [:sepal-width :petal-length :petal-width]\n     {:model-type :smile.regression/elastic-net})\n    (dissoc :model-data))\n\n\n{:feature-columns [:sepal-width :petal-length :petal-width],\n :target-columns [:sepal-length],\n :explained #function[malli.core/-instrument/fn--54083],\n :R2 0.8582120394597336,\n :id #uuid \"a09d5c84-40cf-47a6-8c51-5c392ab56661\",\n :predictions #tech.v3.dataset.column&lt;float64&gt;[150]\n:sepal-length\n[5.022, 4.724, 4.775, 4.851, 5.081, 5.360, 4.911, 5.030, 4.664, 4.903, 5.209, 5.098, 4.775, 4.572, 5.184, 5.522, 5.089, 4.970, 5.352, 5.217...],\n :predict\n #function[scicloj.noj.v1.stats/regression-model/predict--58311],\n :options {:model-type :smile.regression/elastic-net}}\n\n\n(-&gt; iris\n    (stats/regression-model\n     :sepal-length\n     [:sepal-width :petal-length :petal-width]\n     {:model-type :smile.regression/ordinary-least-square})\n    (dissoc :model-data))\n\n\n{:feature-columns [:sepal-width :petal-length :petal-width],\n :target-columns [:sepal-length],\n :explained #function[malli.core/-instrument/fn--54083],\n :R2 0.8586117200664085,\n :id #uuid \"f0475cc4-e3f8-4b04-b45f-8aa9584d3b23\",\n :predictions #tech.v3.dataset.column&lt;float64&gt;[150]\n:sepal-length\n[5.015, 4.690, 4.749, 4.826, 5.080, 5.377, 4.895, 5.021, 4.625, 4.882, 5.216, 5.092, 4.746, 4.533, 5.199, 5.561, 5.094, 4.960, 5.368, 5.226...],\n :predict\n #function[scicloj.noj.v1.stats/regression-model/predict--58311],\n :options {:model-type :smile.regression/ordinary-least-square}}\n\nThe stats/linear-regression-model convenience function uses specifically the :smile.regression/ordinary-least-square model type.\n\n(-&gt; iris\n    (stats/linear-regression-model\n     :sepal-length\n     [:sepal-width :petal-length :petal-width])\n    (dissoc :model-data))\n\n\n{:feature-columns [:sepal-width :petal-length :petal-width],\n :target-columns [:sepal-length],\n :explained #function[malli.core/-instrument/fn--54083],\n :R2 0.8586117200664085,\n :id #uuid \"7f20330a-ec6f-4364-9b03-46fb0bc6a01b\",\n :predictions #tech.v3.dataset.column&lt;float64&gt;[150]\n:sepal-length\n[5.015, 4.690, 4.749, 4.826, 5.080, 5.377, 4.895, 5.021, 4.625, 4.882, 5.216, 5.092, 4.746, 4.533, 5.199, 5.561, 5.094, 4.960, 5.368, 5.226...],\n :predict\n #function[scicloj.noj.v1.stats/regression-model/predict--58311],\n :options {:model-type :smile.regression/ordinary-least-square}}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "stats.html#adding-regression-predictions-to-a-dataset",
    "href": "stats.html#adding-regression-predictions-to-a-dataset",
    "title": "4  Statistics",
    "section": "4.4 Adding regression predictions to a dataset",
    "text": "4.4 Adding regression predictions to a dataset\nThe stats/add-predictions function models a target column using feature columns, adds a new prediction column with the model predictions.\n\n(-&gt; iris\n    (stats/add-predictions\n     :sepal-length\n     [:sepal-width :petal-length :petal-width]\n     {:model-type :smile.regression/ordinary-least-square}))\n\nhttps://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv [150 7]:\n\n\n\n\n\n\n\n\n\n\n\n\n:rownames\n:sepal-length\n:sepal-width\n:petal-length\n:petal-width\n:species\n:sepal-length-prediction\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n5.01541576\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n4.68999718\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n4.74925142\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n4.82599409\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n5.08049948\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n5.37719368\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n4.89468378\n\n\n8\n5.0\n3.4\n1.5\n0.2\nsetosa\n5.02124524\n\n\n9\n4.4\n2.9\n1.4\n0.2\nsetosa\n4.62491347\n\n\n10\n4.9\n3.1\n1.5\n0.1\nsetosa\n4.88164236\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n140\n6.9\n3.1\n5.4\n2.1\nvirginica\n6.53429168\n\n\n141\n6.7\n3.1\n5.6\n2.4\nvirginica\n6.50917327\n\n\n142\n6.9\n3.1\n5.1\n2.3\nvirginica\n6.21025556\n\n\n143\n5.8\n2.7\n5.1\n1.9\nvirginica\n6.17251376\n\n\n144\n6.8\n3.2\n5.9\n2.3\nvirginica\n6.84264484\n\n\n145\n6.7\n3.3\n5.7\n2.5\nvirginica\n6.65460564\n\n\n146\n6.7\n3.0\n5.2\n2.3\nvirginica\n6.21608504\n\n\n147\n6.3\n2.5\n5.0\n1.9\nvirginica\n5.97143313\n\n\n148\n6.5\n3.0\n5.2\n2.0\nvirginica\n6.38302984\n\n\n149\n6.2\n3.4\n5.4\n2.3\nvirginica\n6.61824630\n\n\n150\n5.9\n3.0\n5.1\n1.8\nvirginica\n6.42341317\n\n\n\nIt attaches the model’s information to the metadata of that new column.\n\n(-&gt; iris\n    (stats/add-predictions\n     :sepal-length\n     [:sepal-width :petal-length :petal-width]\n     {:model-type :smile.regression/ordinary-least-square})\n    :sepal-length-prediction\n    meta\n    (update :model\n            dissoc :model-data :predict :predictions))\n\n\n{:name :sepal-length-prediction,\n :datatype :float64,\n :n-elems 150,\n :column-type :prediction,\n :model\n {:feature-columns [:sepal-width :petal-length :petal-width],\n  :target-columns [:sepal-length],\n  :explained #function[malli.core/-instrument/fn--54083],\n  :R2 0.8586117200664085,\n  :id #uuid \"538e67db-da94-44fd-be9e-d49b6c31664b\",\n  :options {:model-type :smile.regression/ordinary-least-square}}}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "stats.html#histograms",
    "href": "stats.html#histograms",
    "title": "4  Statistics",
    "section": "4.5 Histograms",
    "text": "4.5 Histograms\nThe stats/histogram function computes the necessary data to plot a histogram.\n\n(-&gt; (repeatedly 99 rand)\n    (stats/histogram {:bin-count 5}))\n\n_unnamed [5 3]:\n\n\n\n:count\n:left\n:right\n\n\n\n\n22\n0.00274195\n0.20154949\n\n\n22\n0.20154949\n0.40035702\n\n\n25\n0.40035702\n0.59916456\n\n\n14\n0.59916456\n0.79797210\n\n\n16\n0.79797210\n0.99677964\n\n\n\n\nsource: notebooks/stats.clj",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "5  Visualization",
    "section": "",
    "text": "5.1 Visualizing datases with Hanami\nNoj offers a few convenience functions to make Hanami plotting work smoothly with Tablecloth and Kindly.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#visualizing-datases-with-hanami",
    "href": "visualization.html#visualizing-datases-with-hanami",
    "title": "5  Visualization",
    "section": "",
    "text": "(def random-walk\n  (let [n 20]\n    (-&gt; {:x (range n)\n         :y (-&gt;&gt; (repeatedly n #(- (rand) 0.5))\n                 (reductions +))}\n        tc/dataset)))\n\n\n5.1.1 A simple plot\nWe can plot a Tablecloth datasete using a Hanami template:\n\n(-&gt; random-walk\n    (vis.hanami/plot ht/point-chart\n                 {:MSIZE 200}))\n\n\nLet us look inside the resulting vega-lite space. We can see the dataset is included as CSV:\n\n(-&gt; random-walk\n    (vis.hanami/plot ht/point-chart\n                     {:MSIZE 200})\n    kind/pprint)\n\n\n{:encoding\n {:y {:field \"y\", :type \"quantitative\"},\n  :x {:field \"x\", :type \"quantitative\"}},\n :usermeta {:embedOptions {:renderer :svg}},\n :mark {:type \"circle\", :size 200, :tooltip true},\n :width 400,\n :background \"floralwhite\",\n :height 300,\n :data\n {:values\n  \"x,y\\n0,0.18944000598664978\\n1,-0.13094824689977014\\n2,-0.41168493193286537\\n3,-0.208733095687353\\n4,0.18944444410630634\\n5,0.01691569531041648\\n6,-0.011393138062447905\\n7,0.3257381890631419\\n8,0.49366768251626125\\n9,0.30295394177899027\\n10,-0.10436110673634003\\n11,0.13769935965237\\n12,0.25554035399185426\\n13,-0.12821356015550744\\n14,-0.5184935131322956\\n15,-0.5704200309670744\\n16,-0.3927097323124038\\n17,0.08704738652187383\\n18,0.02490965728938943\\n19,0.38205550166238855\\n\",\n  :format {:type \"csv\"}}}\n\n\n\n5.1.2 Additional Hanami templates\nThe scicloj.noj.v1.vis.hanami.templates namespace add Hanami templates to Hanami’s own collection.\n\n(-&gt; datasets/mtcars\n    (vis.hanami/plot vht/boxplot-chart\n                     {:X :gear\n                      :XTYPE :nominal\n                      :Y :mpg}))\n\n\n\n(-&gt; datasets/iris\n    (vis.hanami/plot vht/rule-chart\n                     {:X :sepal-width\n                      :Y :sepal-length\n                      :X2 :petal-width\n                      :Y2 :petal-length\n                      :OPACITY 0.2\n                      :SIZE 3\n                      :COLOR \"species\"}))\n\n\n\n\n5.1.3 Grouped datasets\nGrouped datasets are handled automatically with a table view.\n\n(-&gt; datasets/iris\n    (tc/group-by [:species])\n    (vis.hanami/plot vht/rule-chart\n                     {:X :sepal-width\n                      :Y :sepal-length\n                      :X2 :petal-width\n                      :Y2 :petal-length\n                      :OPACITY 0.2\n                      :SIZE 3}))\n\n\n\n\n\n\n\n\n\n\n:species\n:plot\n\n\n\n\nsetosa\n\n\n\n\n\nversicolor\n\n\n\n\n\nvirginica\n\n\n\n\n\n\n\n\n\n\n5.1.4 Layers\n\n(-&gt; random-walk\n    (vis.hanami/layers\n     {:TITLE \"points and a line\"}\n     [(vis.hanami/plot nil\n                       ht/point-chart\n                       {:MSIZE 400})\n      (vis.hanami/plot nil\n                       ht/line-chart\n                       {:MSIZE 4\n                        :MCOLOR \"brown\"})]))\n\n\nAlternatively:\n\n(-&gt; random-walk\n    (vis.hanami/combined-plot\n     ht/layer-chart\n     {:TITLE \"points and a line\"}\n     :LAYER [[ht/point-chart\n              {:MSIZE 400}]\n             [ht/line-chart\n              {:MSIZE 4\n               :MCOLOR \"brown\"}]]))\n\n\n\n\n5.1.5 Concatenation\nVertical\n\n(-&gt; random-walk\n    (vis.hanami/vconcat\n     {}\n     [(vis.hanami/plot nil\n                       ht/point-chart\n                       {:MSIZE 400\n                        :HEIGHT 100\n                        :WIDTH 100})\n      (vis.hanami/plot nil\n                       ht/line-chart\n                       {:MSIZE 4\n                        :MCOLOR \"brown\"\n                        :HEIGHT 100\n                        :WIDTH 100})]))\n\n\nAlternatively:\n\n(-&gt; random-walk\n    (vis.hanami/combined-plot\n     ht/vconcat-chart\n     {:HEIGHT 100\n      :WIDTH 100}\n     :VCONCAT [[ht/point-chart\n                {:MSIZE 400}]\n               [ht/line-chart\n                {:MSIZE 4\n                 :MCOLOR \"brown\"}]]))\n\n\nHorizontal\n\n(-&gt; random-walk\n    (vis.hanami/hconcat\n     {}\n     [(vis.hanami/plot nil\n                       ht/point-chart\n                       {:MSIZE 400\n                        :HEIGHT 100\n                        :WIDTH 100})\n      (vis.hanami/plot nil\n                       ht/line-chart\n                       {:MSIZE 4\n                        :MCOLOR \"brown\"\n                        :HEIGHT 100\n                        :WIDTH 100})]))\n\n\nAlternatively:\n\n(-&gt; random-walk\n    (vis.hanami/combined-plot\n     ht/hconcat-chart\n     {:HEIGHT 100\n      :WIDTH 100}\n     :HCONCAT [[ht/point-chart\n                {:MSIZE 400}]\n               [ht/line-chart\n                {:MSIZE 4\n                 :MCOLOR \"brown\"}]]))\n\n\n\n:bye\n\n\n:bye\n\n\nsource: notebooks/visualization.clj",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "statistical_visualization.html",
    "href": "statistical_visualization.html",
    "title": "6  Statistical Visualization",
    "section": "",
    "text": "6.1 Linear regression\nAlternatively:\nAnd in a grouped dataset case:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Visualization</span>"
    ]
  },
  {
    "objectID": "statistical_visualization.html#linear-regression",
    "href": "statistical_visualization.html#linear-regression",
    "title": "6  Statistical Visualization",
    "section": "",
    "text": "(-&gt; datasets/mtcars\n    (stats/add-predictions :mpg [:wt]\n                           {:model-type :smile.regression/ordinary-least-square})\n    (vis.hanami/combined-plot\n     ht/layer-chart\n     {:X :wt\n      :MSIZE 200\n      :HEIGHT 200}\n     :LAYER [[ht/point-chart\n              {:Y :mpg\n               :WIDTH 200}]\n             [ht/line-chart\n              {:Y :mpg-prediction\n               :MSIZE 5\n               :MCOLOR \"purple\"\n               :YTITLE :mpg}]]))\n\n\n\n\n(-&gt; datasets/mtcars\n    (vis.stats/linear-regression-plot\n     :mpg :wt\n     {:HEIGHT 200\n      :WIDTH 200\n      :point-options {:MSIZE 200}\n      :line-options {:MSIZE 5\n                     :MCOLOR \"purple\"}}))\n\n\n\n\n(-&gt; datasets/mtcars\n    (tc/group-by [:gear])\n    (vis.stats/linear-regression-plot\n     :mpg :wt\n     {:HEIGHT 200\n      :WIDTH 200\n      :point-options {:MSIZE 200}\n      :line-options {:MSIZE 5\n                     :MCOLOR \"purple\"}}))\n\n\n\n\n\n\n\n\n\n\n:gear\n:plot\n\n\n\n\n4\n\n\n\n\n\n3\n\n\n\n\n\n5",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Visualization</span>"
    ]
  },
  {
    "objectID": "statistical_visualization.html#histogram",
    "href": "statistical_visualization.html#histogram",
    "title": "6  Statistical Visualization",
    "section": "6.2 Histogram",
    "text": "6.2 Histogram\nA histogram groups values in bins, counts them, and creates a corresponding bar-chart.\nThe vis.stats/histogram functions does that behind the scenes, and generates a Vega-Lite spec using Hanami.\n\n(-&gt; datasets/iris\n    (vis.stats/histogram :sepal-width\n                         {:nbins 10}))\n\n\n\n(-&gt; datasets/iris\n    (vis.stats/histogram :sepal-width\n                         {:nbins 10})\n    kind/pprint)\n\n\n{:encoding\n {:y {:field :count, :type \"quantitative\"},\n  :x\n  {:scale {:zero false},\n   :field :left,\n   :type \"quantitative\",\n   :title :sepal-width},\n  :y2 {:field 0, :type \"quantitative\"},\n  :x2 {:scale {:zero false}, :field :right, :type \"quantitative\"}},\n :usermeta {:embedOptions {:renderer :svg}},\n :mark \"rect\",\n :width 400,\n :background \"floralwhite\",\n :height 300,\n :data\n {:values\n  \"count,left,right\\n4,2.0,2.24\\n7,2.24,2.48\\n22,2.48,2.72\\n24,2.72,2.96\\n37,2.96,3.2\\n31,3.2,3.4400000000000004\\n10,3.4400000000000004,3.6800000000000006\\n11,3.6800000000000006,3.9200000000000004\\n2,3.9200000000000004,4.16\\n2,4.16,4.4\\n\",\n  :format {:type \"csv\"}}}\n\nThe resulting spec can be customized further:\n\n(-&gt; datasets/iris\n    (vis.stats/histogram :sepal-width\n                         {:nbins 10})\n    ;; varying the resulting vega-lite spec:\n    (assoc :height 125\n           :width 175))\n\n\n\n:bye\n\n\n:bye\n\n\nsource: notebooks/statistical_visualization.clj",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Visualization</span>"
    ]
  },
  {
    "objectID": "more_visualization.html",
    "href": "more_visualization.html",
    "title": "7  More visualization examples",
    "section": "",
    "text": "7.1 Combining a few things together\nThe following is inspired by the example at Plotnine’s main page. Note how we add regression lines here. We take care of layout and colouring on our side, not using Vega-Lite for that.\nAlternatively, using a grouped dataset:\nA similar example with histograms:\nScatterplots and regression lines again, this time using Vega-Lite for layout and coloring (using its “facet” option).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>More visualization examples</span>"
    ]
  },
  {
    "objectID": "more_visualization.html#combining-a-few-things-together",
    "href": "more_visualization.html#combining-a-few-things-together",
    "title": "7  More visualization examples",
    "section": "",
    "text": "(let [pallete (-&gt;&gt; :accent\n                   color/palette\n                   (mapv color/format-hex))]\n  (-&gt; datasets/mtcars\n      (tc/group-by :gear {:result-type :as-map})\n      (-&gt;&gt; (sort-by key)\n           (map-indexed\n            (fn [i [group-name ds]]\n              (-&gt; ds\n                  (vis.stats/linear-regression-plot\n                   :mpg :wt\n                   {:TITLE (str \"grear=\" group-name)\n                    :X :wt\n                    :MCOLOR (pallete i)\n                    :HEIGHT 200\n                    :WIDTH 200\n                    :point-options {:MSIZE 200}\n                    :line-options {:MSIZE 5}}))))\n           (vis.hanami/vconcat nil {}))))\n\n\n\n\n(let [pallete (-&gt;&gt; :accent\n                   color/palette\n                   (mapv color/format-hex))]\n  (-&gt; datasets/mtcars\n      (tc/map-columns :color [:gear] #(-&gt; % (- 3) pallete))\n      (tc/group-by [:gear])\n      (vis.stats/linear-regression-plot\n       :mpg :wt\n       {:X :wt\n        :MCOLOR {:expr \"datum.color\"}\n        :HEIGHT 200\n        :WIDTH 200\n        :point-options {:MSIZE 200}\n        :line-options {:MSIZE 5}})\n      (tc/order-by [:gear])))\n\n\n\n\n\n\n\n\n\n\n:gear\n:plot\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\n\n\n(let [pallete (-&gt;&gt; :accent\n                   color/palette\n                   (mapv color/format-hex))]\n  (-&gt; datasets/iris\n      (tc/group-by :species {:result-type :as-map})\n      (-&gt;&gt; (sort-by key)\n           (map-indexed\n            (fn [i [group-name ds]]\n              (-&gt; ds\n                  (vis.stats/histogram :sepal-width\n                                       {:nbins 10}))))\n           (vis.hanami/vconcat nil {}))))\n\n\n\n\n(-&gt; datasets/mtcars\n    (tc/group-by [:gear])\n    (stats/add-predictions :mpg [:wt]\n                           {:model-type :smile.regression/ordinary-least-square})\n    (tc/ungroup)\n    (tc/select-columns [:gear :wt :mpg :mpg-prediction])\n    (vis.hanami/combined-plot\n     ht/layer-chart\n     {}\n     :LAYER [[ht/point-chart\n              {:X :wt\n               :Y :mpg\n               :MSIZE 200\n               :COLOR \"gear\"\n               :HEIGHT 100\n               :WIDTH 200}]\n             [ht/line-chart\n              {:X :wt\n               :Y :mpg-prediction\n               :MSIZE 5\n               :COLOR \"gear\"\n               :YTITLE :mpg}]])\n    ((fn [spec]\n       {:facet {:row {:field \"gear\"}}\n        :spec (dissoc spec :data)\n        :data (:data spec)}))\n    kind/vega-lite)\n\n\n\nsource: notebooks/more_visualization.clj",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>More visualization examples</span>"
    ]
  },
  {
    "objectID": "prepare_for_ml.html",
    "href": "prepare_for_ml.html",
    "title": "8  Machine learning specific functionality in tech.ml.dataset",
    "section": "",
    "text": "8.1 Categorical variables\nOne typical problem in machine learning is classification, so learning how to categorize data in different categories. Sometimes data in this format is as well called “qualitative data” or data having discrete values.\nThese categories are often expressed in Clojure as of being of type String or keyword\nIn dataset it is the Column which has specific support for categorical data.\nCreating a column out of categorical data looks like this:\nThis creates a “categorical” column, which is marked as such in the column metadata.\nPrinting the var shows its “type” as being keyword\nand printing its metadata shows that it got marked as categorical\nThe column is therefore using its metadata to store important information, and it is important to get used to look at it for the case of debugging issues.\nThe same happens, when creating a dataset which is a seq of columns\n_unnamed [2 2]:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning specific functionality in `tech.ml.dataset`</span>"
    ]
  },
  {
    "objectID": "prepare_for_ml.html#categorical-variables",
    "href": "prepare_for_ml.html#categorical-variables",
    "title": "8  Machine learning specific functionality in tech.ml.dataset",
    "section": "",
    "text": "(require '[tech.v3.dataset.column :as col]\n         '[tech.v3.dataset :as ds])\n\n\n(def column-x (col/new-column  :x  [:a :b]))\n\n\n\n\ncolumn-x\n\n\n#tech.v3.dataset.column&lt;keyword&gt;[2]\n:x\n[:a, :b]\n\n\n\n(meta column-x)\n\n\n{:categorical? true, :name :x, :datatype :keyword, :n-elems 2}\n\n\n\n\n(def categorical-ds\n  (ds/-&gt;dataset\n   {:x [:a :b] :y [\"c\" \"d\"]}))\n\n\ncategorical-ds\n\n\n\n\n\n:x\n:y\n\n\n\n\n:a\nc\n\n\n:b\nd\n\n\n\n\n(map\n meta\n (vals categorical-ds))\n\n\n({:categorical? true, :name :x, :datatype :keyword, :n-elems 2}\n {:categorical? true, :name :y, :datatype :string, :n-elems 2})\n\n\n8.1.1 Transform categorical variables to numerical space\nMost machine learning models can only work on numerical values, both for features and the target variable. So usually we need to transform categorical data into a numeric representation, so each category need to be converted to a number.\nThese numbers have often no meaning for the users, so often we need to convert back into String / keyword space later on.\nNamespace tech.v3.dataset.categorical has several functions to do so.\n\n\n8.1.2 Transform categorical column into a numerical column\n\n(require  '[tech.v3.dataset.categorical :as ds-cat])\n\nThese functions operate on a single column, but expect a dataset and a column name as input.\nWe use them to calculate a mapping from string/keyword to a numerical space (0 … x) like this\n\n(ds-cat/fit-categorical-map categorical-ds :x)\n\n\n{:lookup-table {:a 0, :b 1}, :src-column :x, :result-datatype :float64}\n\nThis maps the values in their order of occurrence in the column to 0 .. 1 This is a bit dangerous, as the mapping is decided by “row order”, which could change or be different on other subset of the data, like test/train splits\nSo it is preferred to be specified explicitly.\n\n(def x-mapping (ds-cat/fit-categorical-map categorical-ds :x [:a :b]))\n\n\nx-mapping\n\n\n{:lookup-table {:a 0, :b 1}, :src-column :x, :result-datatype :float64}\n\nNow we know for sure, that :a is mapped to 0 and :b is mapped to 1. Once we have a mapping, we can use it on new data and transform it into numerical values\n\n(def numerical-categorical-data\n  (ds-cat/transform-categorical-map\n   (ds/-&gt;dataset {:x [:a :b :a :b :b :b]})\n   x-mapping))\n\n\nnumerical-categorical-data\n\n_unnamed [6 1]:\n\n\n\n:x\n\n\n\n\n0.0\n\n\n1.0\n\n\n0.0\n\n\n1.0\n\n\n1.0\n\n\n1.0\n\n\n\nWe can revert it as well:\n\n(ds-cat/invert-categorical-map numerical-categorical-data x-mapping)\n\n_unnamed [6 1]:\n\n\n\n:x\n\n\n\n\n:a\n\n\n:b\n\n\n:a\n\n\n:b\n\n\n:b\n\n\n:b\n\n\n\nWe can as well ask about all mapping of a dataset:\n\n(ds-cat/dataset-&gt;categorical-maps numerical-categorical-data)\n\n\n({:lookup-table {:a 0, :b 1},\n  :src-column :x,\n  :result-datatype :float64})",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning specific functionality in `tech.ml.dataset`</span>"
    ]
  },
  {
    "objectID": "prepare_for_ml.html#convert-several-columns-in-one-go",
    "href": "prepare_for_ml.html#convert-several-columns-in-one-go",
    "title": "8  Machine learning specific functionality in tech.ml.dataset",
    "section": "8.2 Convert several columns in one go",
    "text": "8.2 Convert several columns in one go\nThe dataset namespace has a convenience function in which several columns can be selected for conversion.\n\n(ds/categorical-&gt;number categorical-ds [:x :y])\n\n_unnamed [2 2]:\n\n\n\n:x\n:y\n\n\n\n\n0.0\n1.0\n\n\n1.0\n0.0\n\n\n\nThis works as well with filter function from namespace column-filters\n\n(require '[tech.v3.dataset.column-filters :as ds-cf])\n\nto convert all categorical columns, for example:\n\n(ds/categorical-&gt;number categorical-ds ds-cf/categorical)\n\n_unnamed [2 2]:\n\n\n\n:x\n:y\n\n\n\n\n0.0\n1.0\n\n\n1.0\n0.0\n\n\n\n\n(-&gt;\n (ds/-&gt;dataset {:x [:a :b]\n                :y [:c :d]})\n (ds/categorical-&gt;number [:x :y] [:a :b :c :d]))\n\n_unnamed [2 2]:\n\n\n\n:x\n:y\n\n\n\n\n0.0\n2.0\n\n\n1.0\n3.0\n\n\n\n\n(-&gt;\n (ds/-&gt;dataset {:x [:a :b]\n                :y [:c :d]})\n (ds/categorical-&gt;number [:x :y] [:a 0 :b 1 :c 2 :d 3]))\n\n_unnamed [2 2]:\n\n\n\n:x\n:y\n\n\n\n\n0.0\n4.0\n\n\n2.0\n6.0",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning specific functionality in `tech.ml.dataset`</span>"
    ]
  },
  {
    "objectID": "prepare_for_ml.html#warning-pitfalls-of-categorical-maps",
    "href": "prepare_for_ml.html#warning-pitfalls-of-categorical-maps",
    "title": "8  Machine learning specific functionality in tech.ml.dataset",
    "section": "8.3 Warning: Pitfalls of Categorical maps",
    "text": "8.3 Warning: Pitfalls of Categorical maps\n\n8.3.1 Automatic mapping might result in surprising results\nWe need to be careful when visually inspecting columns without reverting the categorical maps.\nApplying the following map to a dataset\n\n(ds-cat/fit-categorical-map (ds/-&gt;dataset {:x [\"true\" \"false\" ]}) :x)\n\n\n{:lookup-table {\"true\" 0, \"false\" 1},\n :src-column :x,\n :result-datatype :float64}\n\nwould result in columns in which ‘0’ would mean ‘true’, and ‘1’ would mean ‘false’\n\n\n8.3.2 float vs int\nThe categories can get mapped to int or float\n\n(def ds-with-float-and-int-mappings\n  (-&gt;\n   (ds/-&gt;dataset {:x-float [:a :b]\n                  :x-int [:a :b]})\n   (ds/categorical-&gt;number [:x-float] [] :float64)\n   (ds/categorical-&gt;number [:x-int]   [] :int)))\n\nComparing such columns might not bring the expected result, even though the categorical maps and values look very similar\n\nds-with-float-and-int-mappings\n\n_unnamed [2 2]:\n\n\n\n:x-float\n:x-int\n\n\n\n\n0.0\n0\n\n\n1.0\n1\n\n\n\n\n(map meta\n     (vals ds-with-float-and-int-mappings))\n\n\n({:categorical? true,\n  :name :x-float,\n  :datatype :float64,\n  :n-elems 2,\n  :categorical-map\n  {:lookup-table {:a 0, :b 1},\n   :src-column :x-float,\n   :result-datatype :float64}}\n {:categorical? true,\n  :name :x-int,\n  :datatype :int,\n  :n-elems 2,\n  :categorical-map\n  {:lookup-table {:a 0, :b 1},\n   :src-column :x-int,\n   :result-datatype :int}})\n\n\n\n8.3.3 Categorical maps attached to a column change semantic value of the Column\nThe existence of categorical maps on a column, change the semantic value of the data. When categorical maps are different for two columns (for whatever reasons), it is not given that the column cell value like 0 means the same in both columns. Columns which have categorical maps should never be compared via clojure.core/= as this will ignore the categorical maps. (unless we are sure that the categorical maps in both are the same) They should be converted back to their original space and then compared. This is specially important for comparing prediction and true value in machine learning for metric calculations.\nSee the following example to illustrate this.\n\n8.3.3.1 Incorrect comparisons\nIn the following the two columns are clearly different (the opposite even)\n\n(def ds-with-different-cat-maps\n  (-&gt;\n   (ds/-&gt;dataset {:x-1 [:a :b :a :b :b :b]\n                  :x-2 [:b :a :b :a :a :a]})\n   (ds/categorical-&gt;number [:x-1 :x-2])))\n\nThe resulting columns look the same, but are not\n\n(:x-1 ds-with-different-cat-maps)\n\n\n#tech.v3.dataset.column&lt;float64&gt;[6]\n:x-1\n[0, 1, 0, 1, 1, 1]\n\n\n(:x-2 ds-with-different-cat-maps)\n\n\n#tech.v3.dataset.column&lt;float64&gt;[6]\n:x-2\n[0, 1, 0, 1, 1, 1]\n\nBy using default categorical-&gt;number we get different categorical maps, having different :lookup-tables\n\n(meta (:x-1 ds-with-different-cat-maps))\n\n\n{:categorical? true,\n :name :x-1,\n :datatype :float64,\n :n-elems 6,\n :categorical-map\n {:lookup-table {:a 0, :b 1},\n  :src-column :x-1,\n  :result-datatype :float64}}\n\n\n(meta (:x-2 ds-with-different-cat-maps))\n\n\n{:categorical? true,\n :name :x-2,\n :datatype :float64,\n :n-elems 6,\n :categorical-map\n {:lookup-table {:b 0, :a 1},\n  :src-column :x-2,\n  :result-datatype :float64}}\n\nso they are (wrongly) compared as equal\n\n(=\n (:x-1 ds-with-different-cat-maps)\n (:x-2 ds-with-different-cat-maps))\n\n\ntrue\n\n\n\n8.3.3.2 Correct comparison\nIn order to compare them correctly, we need to first revert the categorical mappings\n\n(def reverted-ds-with-different-cat-maps\n  (ds-cat/reverse-map-categorical-xforms ds-with-different-cat-maps))\n\n\n(:x-1 reverted-ds-with-different-cat-maps)\n\n\n#tech.v3.dataset.column&lt;keyword&gt;[6]\n:x-1\n[:a, :b, :a, :b, :b, :b]\n\n\n(:x-2 reverted-ds-with-different-cat-maps)\n\n\n#tech.v3.dataset.column&lt;keyword&gt;[6]\n:x-2\n[:b, :a, :b, :a, :a, :a]\n\nand now they compare correctly as :false\n\n(=\n (:x-1 reverted-ds-with-different-cat-maps)\n (:x-2 reverted-ds-with-different-cat-maps))\n\n\nfalse\n\nSo it should be as well avoided to transform mapped columns to other representations, which loose the mappings, like tensor or primitive arrays, or even sequences\n\n\n8.3.3.3 Use the same and fixed mapping\nThis issue can be avoided by specifying concretely the mapping to be used, as being for example {:a 0 :b 1}\n\n(def ds-with-same-cat-maps\n  (-&gt;\n   (ds/-&gt;dataset {:x-1 [:a :b :a :b :b :b]\n                  :x-2 [:b :a :b :a :a :a]})\n   (ds/categorical-&gt;number [:x-1 :x-2] [:a :b])))\n\nmapping spec can be either [:a :b] or [:a 0 :b 1]\n\n(:x-1 ds-with-same-cat-maps)\n\n\n#tech.v3.dataset.column&lt;float64&gt;[6]\n:x-1\n[0, 1, 0, 1, 1, 1]\n\n\n(:x-2 ds-with-same-cat-maps)\n\n\n#tech.v3.dataset.column&lt;float64&gt;[6]\n:x-2\n[1, 0, 1, 0, 0, 0]\n\nwe get same categorical maps\n\n(meta (:x-1 ds-with-same-cat-maps))\n\n\n{:categorical? true,\n :name :x-1,\n :datatype :float64,\n :n-elems 6,\n :categorical-map\n {:lookup-table {:a 0, :b 1},\n  :src-column :x-1,\n  :result-datatype :float64}}\n\n\n(meta (:x-2 ds-with-same-cat-maps))\n\n\n{:categorical? true,\n :name :x-2,\n :datatype :float64,\n :n-elems 6,\n :categorical-map\n {:lookup-table {:a 0, :b 1},\n  :src-column :x-2,\n  :result-datatype :float64}}\n\nso they are correctly compared as not equal\n\n(=\n (:x-1 ds-with-same-cat-maps)\n (:x-2 ds-with-same-cat-maps))\n\n\nfalse\n\nThese 3 pitfalls can be avoided by explicitly specifying the mappings, so using the 4-arity of conversion functions.\n\n(def ds-with-explicit-mapping\n  (-&gt;\n   (ds/-&gt;dataset {:x-1 [:a :b :a :b :b :b]\n                  :x-2 [:b :a :b :a :a :a]})\n   (ds/categorical-&gt;number [:x-1 :x-2] [:a :b] :int)))\n\n\nds-with-explicit-mapping\n\n_unnamed [6 2]:\n\n\n\n:x-1\n:x-2\n\n\n\n\n0\n1\n\n\n1\n0\n\n\n0\n1\n\n\n1\n0\n\n\n1\n0\n\n\n1\n0\n\n\n\n\n(map meta (vals ds-with-explicit-mapping))\n\n\n({:categorical? true,\n  :name :x-1,\n  :datatype :int,\n  :n-elems 6,\n  :categorical-map\n  {:lookup-table {:a 0, :b 1},\n   :src-column :x-1,\n   :result-datatype :int}}\n {:categorical? true,\n  :name :x-2,\n  :datatype :int,\n  :n-elems 6,\n  :categorical-map\n  {:lookup-table {:a 0, :b 1},\n   :src-column :x-2,\n   :result-datatype :int}})\n\n\n\n\n8.3.4 one-hot-encoding\nFor some models / use cases the categorical data need to be converted in the so called one-hot format. In this every column get multiplied by the number of categories , and then each one-hot column can only have 0 and 1 values.\n\n(def one-hot-map-x (ds-cat/fit-one-hot categorical-ds :x))\n\n\n(def one-hot-map-y (ds-cat/fit-one-hot categorical-ds :y))\n\n\none-hot-map-x\n\n\n{:one-hot-table {:a :x-a, :b :x-b},\n :src-column :x,\n :result-datatype :float64}\n\n\none-hot-map-y\n\n\n{:one-hot-table {\"d\" :y-d, \"c\" :y-c},\n :src-column :y,\n :result-datatype :float64}\n\n\ncategorical-ds\n\n_unnamed [2 2]:\n\n\n\n:x\n:y\n\n\n\n\n:a\nc\n\n\n:b\nd\n\n\n\nget transformed by\n\n(def one-hot-ds\n  (-&gt; categorical-ds\n      (ds-cat/transform-one-hot one-hot-map-x)\n      (ds-cat/transform-one-hot one-hot-map-y)))\n\ninto\n\none-hot-ds\n\n_unnamed [2 4]:\n\n\n\n:x-a\n:x-b\n:y-d\n:y-c\n\n\n\n\n1.0\n0.0\n0.0\n1.0\n\n\n0.0\n1.0\n1.0\n0.0\n\n\n\nThere are similar functions to convert this format back.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning specific functionality in `tech.ml.dataset`</span>"
    ]
  },
  {
    "objectID": "prepare_for_ml.html#features-and-inference-target-in-a-dataset",
    "href": "prepare_for_ml.html#features-and-inference-target-in-a-dataset",
    "title": "8  Machine learning specific functionality in tech.ml.dataset",
    "section": "8.4 Features and inference target in a dataset",
    "text": "8.4 Features and inference target in a dataset\nA dataset for supervised machine learning has always two groups of columns. They can either be the features or the inference targets. The goal of the learning is to find the relationship between the two groups and therefore be able to predict inference targets from features. Sometimes the features are called X and the targets y.\nWhen constructing a dataset\n\n(def ds\n  (ds/-&gt;dataset {:x-1 [0 1 0]\n                 :x-2 [1 0 1]\n                 :y [:a :a :b]}))\n\nwe need to mark explicitly which columns are features and which are targets in order to be able to use the dataset later for machine learning in metamorph.ml\nAs normally only one or a few columns are inference targets, we can simply mark those and the other columns are regarded as features.\n\n(require  '[tech.v3.dataset.modelling :as ds-mod])\n\n\n(def modelled-ds\n  (-&gt; ds\n      (ds-mod/set-inference-target :y)))\n\n(works as well with a seq)\nThis is marked as well in the column metadata.\n\n(-&gt; modelled-ds :y meta)\n\n\n{:categorical? true,\n :name :y,\n :datatype :keyword,\n :n-elems 3,\n :inference-target? true}\n\nThere are several functions to get information on features and inference targets:\n\n(ds-mod/feature-ecount modelled-ds)\n\n\n3\n\n\n(ds-cf/feature modelled-ds)\n\n_unnamed [3 2]:\n\n\n\n:x-1\n:x-2\n\n\n\n\n0\n1\n\n\n1\n0\n\n\n0\n1\n\n\n\n\n(ds-cf/target modelled-ds)\n\n_unnamed [3 1]:\n\n\n\n:y\n\n\n\n\n:a\n\n\n:a\n\n\n:b",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning specific functionality in `tech.ml.dataset`</span>"
    ]
  },
  {
    "objectID": "prepare_for_ml.html#combining-categorical-transformation-and-modelling",
    "href": "prepare_for_ml.html#combining-categorical-transformation-and-modelling",
    "title": "8  Machine learning specific functionality in tech.ml.dataset",
    "section": "8.5 Combining categorical transformation and modelling",
    "text": "8.5 Combining categorical transformation and modelling\nVery often we need to do transform and model for doing classification and combine the -&gt;numeric transformation of categorical vars and the marking of inference targets.\n\n(def ds-ready-for-train\n  (-&gt;\n   {:x-1 [0 1 0]\n    :x-2 [1 0 1]\n    :cat  [:a :b :c]\n    :y [:a :a :b]}\n\n   (ds/-&gt;dataset)\n   (ds/categorical-&gt;number [:y])\n   (ds/categorical-&gt;one-hot [:cat])\n   (ds-mod/set-inference-target [:y])))\n\n\nds-ready-for-train\n\n_unnamed [3 6]:\n\n\n\n:x-1\n:x-2\n:y\n:cat-c\n:cat-a\n:cat-b\n\n\n\n\n0\n1\n0.0\n0.0\n1.0\n0.0\n\n\n1\n0\n0.0\n0.0\n0.0\n1.0\n\n\n0\n1\n1.0\n1.0\n0.0\n0.0\n\n\n\nSuch a dataset is ready for training as it only contains numerical variables which have the categorical maps in place for easy converting back, if needed. The inference target is marked as well, as we can see in the meta data:\n\n(map meta (vals ds-ready-for-train))\n\n\n({:name :x-1, :datatype :int64, :n-elems 3}\n {:name :x-2, :datatype :int64, :n-elems 3}\n {:categorical? true,\n  :name :y,\n  :datatype :float64,\n  :n-elems 3,\n  :categorical-map\n  {:lookup-table {:a 0, :b 1},\n   :src-column :y,\n   :result-datatype :float64},\n  :inference-target? true}\n {:categorical? true,\n  :name :cat-c,\n  :datatype :float64,\n  :n-elems 3,\n  :one-hot-map\n  {:one-hot-table {:c :cat-c, :a :cat-a, :b :cat-b},\n   :src-column :cat,\n   :result-datatype :float64}}\n {:categorical? true,\n  :name :cat-a,\n  :datatype :float64,\n  :n-elems 3,\n  :one-hot-map\n  {:one-hot-table {:c :cat-c, :a :cat-a, :b :cat-b},\n   :src-column :cat,\n   :result-datatype :float64}}\n {:categorical? true,\n  :name :cat-b,\n  :datatype :float64,\n  :n-elems 3,\n  :one-hot-map\n  {:one-hot-table {:c :cat-c, :a :cat-a, :b :cat-b},\n   :src-column :cat,\n   :result-datatype :float64}})\n\nMost models in the metamorph.ml ecosystem can work with data in this format.\nSide remark: If needed, data could as well be easily transformed into a tensor. Most models do this internally anyway (often to primitive arrays)\n\n(def ds-tensor\n  (tech.v3.dataset.tensor/dataset-&gt;tensor ds-ready-for-train))\n\n\nds-tensor\n\n\n#tech.v3.tensor&lt;float64&gt;[3 6]\n[[0.000 1.000 0.000 0.000 1.000 0.000]\n [1.000 0.000 0.000 0.000 0.000 1.000]\n [0.000 1.000 1.000 1.000 0.000 0.000]]\n\nor we can do so, if needed, but this looses the notation of features / inference target\n\n(tech.v3.tensor/-&gt;jvm ds-tensor)\n\n\n[[0.0 1.0 0.0 0.0 1.0 0.0]\n [1.0 0.0 0.0 0.0 0.0 1.0]\n [0.0 1.0 1.0 1.0 0.0 0.0]]\n\n\nsource: notebooks/prepare_for_ml.clj",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Machine learning specific functionality in `tech.ml.dataset`</span>"
    ]
  },
  {
    "objectID": "ml_basic.html",
    "href": "ml_basic.html",
    "title": "9  Machine learning",
    "section": "",
    "text": "9.1 Inspect data\nThe titanic data is part of metamorph.ml and in the form of a train, test split\nWe use the :train part only for this tutorial.\nthis is the full dataset\n_unnamed [5 12]:\nIt has various columns\nof which we can get some statistics\n_unnamed: descriptive-stats [12 12]:\nThe data is more or less balanced across the 2 classes:\nWe will make a very simple model, which will predict the column :survived from columns :sex , :pclass and :embark These represent the “gender”, “passenger class” and “port of embarkment”",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "ml_basic.html#inspect-data",
    "href": "ml_basic.html#inspect-data",
    "title": "9  Machine learning",
    "section": "",
    "text": "(defonce titanic-split\n  (data/titanic-ds-split))\n\n\n\n(def titanic\n  (:train titanic-split))\n\n\n(tc/head\n titanic)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:passenger-id\n:survived\n:pclass\n:name\n:sex\n:age\n:sib-sp\n:parch\n:ticket\n:fare\n:cabin\n:embarked\n\n\n\n\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\n\nS\n\n\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\n\nS\n\n\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\n\nS\n\n\n\n\n\n(tc/column-names titanic)\n\n\n(:passenger-id\n :survived\n :pclass\n :name\n :sex\n :age\n :sib-sp\n :parch\n :ticket\n :fare\n :cabin\n :embarked)\n\n\n\n(ds/descriptive-stats titanic)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:col-name\n:datatype\n:n-valid\n:n-missing\n:min\n:mean\n:mode\n:max\n:standard-deviation\n:skew\n:first\n:last\n\n\n\n\n:passenger-id\n:int16\n891\n0\n1.00\n446.00000000\n\n891.0000\n257.35384202\n0.00000000\n1\n891\n\n\n:survived\n:int16\n891\n0\n0.00\n0.38383838\n\n1.0000\n0.48659245\n0.47852344\n0\n0\n\n\n:pclass\n:int16\n891\n0\n1.00\n2.30864198\n\n3.0000\n0.83607124\n-0.63054791\n3\n3\n\n\n:name\n:string\n891\n0\n\n\nMallet, Mr. Albert\n\n\n\nBraund, Mr. Owen Harris\nDooley, Mr. Patrick\n\n\n:sex\n:string\n891\n0\n\n\nmale\n\n\n\nmale\nmale\n\n\n:age\n:float64\n714\n177\n0.42\n29.69911765\n\n80.0000\n14.52649733\n0.38910778\n22.00\n32.00\n\n\n:sib-sp\n:int16\n891\n0\n0.00\n0.52300786\n\n8.0000\n1.10274343\n3.69535173\n1\n0\n\n\n:parch\n:int16\n891\n0\n0.00\n0.38159371\n\n6.0000\n0.80605722\n2.74911705\n0\n0\n\n\n:ticket\n:string\n891\n0\n\n\nCA. 2343\n\n\n\nA/5 21171\n370376\n\n\n:fare\n:float64\n891\n0\n0.00\n32.20420797\n\n512.3292\n49.69342860\n4.78731652\n7.250\n7.750\n\n\n:cabin\n:string\n204\n687\n\n\n\n\n\n\n\n\n\n\n:embarked\n:string\n889\n2\n\n\nS\n\n\n\nS\nQ\n\n\n\n\n\n(-&gt; titanic :survived frequencies)\n\n\n{0 549, 1 342}\n\n\n\n(def categorical-feature-columns [:sex :pclass :embarked])\n\n\n(def target-column :survived)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "ml_basic.html#convert-categorical-features-to-numeric",
    "href": "ml_basic.html#convert-categorical-features-to-numeric",
    "title": "9  Machine learning",
    "section": "9.2 Convert categorical features to numeric",
    "text": "9.2 Convert categorical features to numeric\nAs we need to convert the non numerical feature columns to categorical, we will first look at their unique values:\n\n(map\n #(hash-map\n   :col-name %\n   :values  (distinct (get titanic %)))\n categorical-feature-columns)\n\n\n({:col-name :sex, :values (\"male\" \"female\")}\n {:col-name :pclass, :values (3 1 2)}\n {:col-name :embarked, :values (\"S\" \"C\" \"Q\" nil)})\n\nThis allows us now to set specifically the values in the conversion to numbers. This is a good practice, instead of the relying on the automatic selection of the categorical mapping:\n\n(require\n         '[tech.v3.dataset.categorical :as ds-cat]\n         '[tech.v3.dataset.modelling :as ds-mod]\n         '[tech.v3.dataset.column-filters :as cf])\n\nThis gives then the selected and numeric columns like this:\n\n(def relevant-titanic-data\n  (-&gt; titanic\n      (tc/select-columns (conj categorical-feature-columns target-column))\n      (ds/drop-missing)\n      (ds/categorical-&gt;number [:survived] [0 1] :float64)\n      (ds-mod/set-inference-target target-column)))\n\nof which we can inspect the lookup-tables\n\n(def cat-maps\n  [(ds-cat/fit-categorical-map relevant-titanic-data :sex [\"male\" \"female\"] :float64)\n   (ds-cat/fit-categorical-map relevant-titanic-data :pclass [0 1 2] :float64)\n   (ds-cat/fit-categorical-map relevant-titanic-data :embarked [\"S\" \"Q\" \"C\"] :float64)])\n\n\ncat-maps\n\n\n[{:lookup-table {\"male\" 0, \"female\" 1},\n  :src-column :sex,\n  :result-datatype :float64}\n {:lookup-table {0 0, 1 1, 2 2, 3 3},\n  :src-column :pclass,\n  :result-datatype :float64}\n {:lookup-table {\"S\" 0, \"Q\" 1, \"C\" 2},\n  :src-column :embarked,\n  :result-datatype :float64}]\n\nAfter the mappings are applied, we have a numeric dataset, as expected by most models.\n\n(def numeric-titanic-data\n  (reduce (fn [ds cat-map]\n            (ds-cat/transform-categorical-map ds cat-map))\n          relevant-titanic-data\n          cat-maps))\n\n\n(tc/head\n numeric-titanic-data)\n\n_unnamed [5 4]:\n\n\n\n:sex\n:pclass\n:embarked\n:survived\n\n\n\n\n0.0\n3.0\n0.0\n0.0\n\n\n1.0\n1.0\n2.0\n1.0\n\n\n1.0\n3.0\n0.0\n1.0\n\n\n1.0\n1.0\n0.0\n1.0\n\n\n0.0\n3.0\n0.0\n0.0\n\n\n\nSplit data into train and test set Now we split the data into train and test. By we use a :holdout strategy, so will get a single split in training an test data.\n\n(def split\n  (first\n   (tc/split-&gt;seq numeric-titanic-data :holdout {:seed 112723})))\n\n\nsplit\n\n\n{\n\n\n\n\n\n\n\n\n:train\n\n\nGroup: 0 [592 4]:\n:sex:pclass:embarked:survived\n0.03.02.00.0\n0.03.00.00.0\n0.03.00.00.0\n1.03.02.01.0\n0.01.00.00.0\n1.03.00.00.0\n1.02.00.01.0\n1.03.00.00.0\n0.03.00.00.0\n1.01.02.01.0\n…………\n0.03.00.00.0\n1.02.00.01.0\n0.03.02.01.0\n1.02.00.00.0\n0.02.00.00.0\n1.03.00.01.0\n0.02.00.00.0\n1.02.00.01.0\n0.03.00.01.0\n0.03.00.00.0\n0.03.01.00.0\n\n\n\n\n\n\n\n\n\n\n\n\n:test\n\n\nGroup: 0 [297 4]:\n:sex:pclass:embarked:survived\n0.01.00.00.0\n1.03.00.00.0\n0.02.00.00.0\n0.03.00.00.0\n0.03.00.01.0\n0.03.00.00.0\n0.03.00.00.0\n0.03.00.00.0\n1.03.02.01.0\n0.01.02.00.0\n…………\n0.03.00.00.0\n1.03.00.00.0\n0.03.00.00.0\n1.03.01.01.0\n0.03.00.00.0\n1.02.02.01.0\n0.03.00.00.0\n0.03.00.00.0\n0.03.00.01.0\n0.01.00.00.0\n1.01.02.01.0\n\n\n\n\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "ml_basic.html#train-a-model",
    "href": "ml_basic.html#train-a-model",
    "title": "9  Machine learning",
    "section": "9.3 Train a model",
    "text": "9.3 Train a model\nNow its time to train a model.\n\n(require '[scicloj.metamorph.ml :as ml]\n         '[scicloj.metamorph.ml.classification]\n         '[scicloj.metamorph.ml.loss :as loss])\n\n\n9.3.1 Dummy model\nWe start with a dummy model, which simply predicts the majority class\n\n(def dummy-model (ml/train (:train split) {:model-type :metamorph.ml/dummy-classifier}))\n\n\n(def dummy-prediction\n  (ml/predict (:test split) dummy-model))\n\nIt always predicts a single class, as expected:\n\n(-&gt; dummy-prediction :survived frequencies)\n\n\n{1.0 297}\n\nwe can calculate accuracy by using a metric after having converted the numerical data back to original (important !) We should never compare mapped columns directly.\n\n(loss/classification-accuracy\n (:survived (ds-cat/reverse-map-categorical-xforms (:test split)))\n (:survived (ds-cat/reverse-map-categorical-xforms dummy-prediction)))\n\n\n0.3973063973063973\n\nIt’s performance is poor, even worse the coin flip.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "ml_basic.html#logistic-regression",
    "href": "ml_basic.html#logistic-regression",
    "title": "9  Machine learning",
    "section": "9.4 Logistic regression",
    "text": "9.4 Logistic regression\nNext model to use is Logistic Regression\n\n(require '[scicloj.ml.smile.classification])\n\n\n(def lreg-model (ml/train (:train split) {:model-type :smile.classification/logistic-regression}))\n\n\n(def lreg-prediction\n  (ml/predict (:test split) lreg-model))\n\n\n(loss/classification-accuracy\n (:survived (ds-cat/reverse-map-categorical-xforms (:test split)))\n (:survived (ds-cat/reverse-map-categorical-xforms lreg-prediction)))\n\n\n0.7373737373737373\n\nIts performance is better, 60 %",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "ml_basic.html#random-forrest",
    "href": "ml_basic.html#random-forrest",
    "title": "9  Machine learning",
    "section": "9.5 Random forrest",
    "text": "9.5 Random forrest\nNext is random forrest\n\n(def rf-model (ml/train (:train split) {:model-type :smile.classification/random-forest}))\n\n\n(def rf-prediction\n  (ml/predict (:test split) rf-model))\n\n\n(loss/classification-accuracy\n (:survived (ds-cat/reverse-map-categorical-xforms (:test split)))\n (:survived (ds-cat/reverse-map-categorical-xforms rf-prediction)))\n\n\n0.7676767676767677\n\nbest so far, 71 %\nFrom the logistic regression model we can get via java Interop some model explanations, for example the variable importance.\n\n(-&gt;&gt;\n (map\n  (fn [predictor importance]\n    (hash-map :predictor (-&gt; predictor str csk/-&gt;kebab-case-keyword)\n              :importance importance))\n\n  (-&gt; rf-model ml/thaw-model .formula .predictors)\n  (-&gt; rf-model ml/thaw-model .importance))\n (sort-by :importance)\n reverse)\n\n\n({:predictor :sex, :importance 42.307675668456376}\n {:predictor :pclass, :importance 9.694046050097613}\n {:predictor :embarked, :importance 1.7705349924162808})\n\nwe can see that :sex is more important to predict :survived then :pclass and :embark",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Machine learning</span>"
    ]
  },
  {
    "objectID": "automl.html",
    "href": "automl.html",
    "title": "10  AutoML using metamorph pipelines",
    "section": "",
    "text": "10.1 The metamorph pipeline abstraction\nFor doing automl, it is very useful to be able to handle the steps of machine learning pipeline (so data transformations and modeling) as a single function which can be moved around freely. This cannot work with a threading macro, as this executes immediate.\nThe Clojure way to do this, is function composing and higher level functions\n(The following is a very low level explanation of metamorph, as a metamorph.ml user we do not use this low-level functions , see next chapter)\nWhile before we saw how to use the pair of train and predict to perform machine learning, AutoML requires us to use an other abstraction, in order to encapsulate both train and predict in single function.(or other any operation)\nWe will use the concept of a “metamorph pipeline”, which is a sequence of specific functions, and each function can behaves differently, depending on the “mode” in which the pipelines get run. It can run either in mode :fit or in mode :transform, and the functions of the pipeline can (but don’t need to) do different things depend on the mode\nSpecifically we have a function called metamorph.ml/model which will do train in mode :fit and predict in mode :transform\nThe names :fit and :transform come from the fact that functions could do other things then train and predict, so :fit and :transform represent a more general concept then train/predict\nWe will use the ready-for-modeling data from basic-ml tutorial,\nso lets create splits of the data first.\nIn its foundation a metamorph pipeline is a sequential composition of functions, which all take a map as only parameter, the so called context, and they return an other context, changed by the functions. The composed function , hence the pipeline overall, has this same property. Any other function parameters are closed over on function creation. The following creates such a composed function out of other metamorph compliant operations. The overall result of the pipeline function, is the result of the last operation. (in this case we have only ‘1’ operation)\nIn nearly all cases, the last pipeline operation is ml/model . But this is not absolutely required.\nas we see, this is a function itself\nThis function is metamorph compliant, so it takes a map (my-pipeline {}) and returns a map.\nBut this map cannot be “arbitrary”, it need to adhere to the metamorph conventions.\nThe following trains a model, because the ml/model function does this when called with :mode :fit And it is the only operation in the pipeline, so the pipeline does one thing, it trains a model\nThis context map has the “data”, the “mode” and an UUID for each operation (we had only one in this pipeline)\nThe model function has closed over the id, so is knows “his id”, so in the transform mode it can get the data created at :fit. So the model function can “send” data to itself from :fit to :transform, the trained model.\nSo this will do the predict on new data\nFor the dummy-model we do not see a trained-model, but it “communicates” the majority class from the train data to use it for prediction. So the dummy-model has ‘learned’ the majority class from its training data.\nSo we can get prediction result out of the ctx:\nThis works as long as all operations of the pipeline follow the metamorph convention (we can create such compliant functions, out of normal dataset-&gt;dataset functions, as we will see)\nmy-pipeline represents therefore a not yet executed model training / prediction. It can be freely moved around and applied to a dataset when needed.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AutoML using metamorph pipelines</span>"
    ]
  },
  {
    "objectID": "automl.html#the-metamorph-pipeline-abstraction",
    "href": "automl.html#the-metamorph-pipeline-abstraction",
    "title": "10  AutoML using metamorph pipelines",
    "section": "",
    "text": "(require '[scicloj.metamorph.ml :as ml]\n         '[scicloj.metamorph.core :as mm]\n         '[tablecloth.api :as tc])\n\n\n\n(def titanic ml-basic/numeric-titanic-data)\n\n\n\n(def splits (first (tc/split-&gt;seq titanic)))\n\n\n(def train-ds (:train splits))\n\n\n(def test-ds (:test splits))\n\n\n\n\n(def my-pipeline\n  (mm/pipeline\n   (ml/model {:model-type :metamorph.ml/dummy-classifier})))\n\n\n\nmy-pipeline\n\n\n#function[scicloj.metamorph.core/pipeline/local-pipeline--49861]\n\n\n\n\n\n(def ctx-after-train\n  (my-pipeline {:metamorph/data train-ds\n                :metamorph/mode :fit}))\n\n\nctx-after-train\n\n\n{\n\n\n\n\n\n\n\n\n:metamorph/data\n\n\nGroup: 0 [711 4]:\n:sex:pclass:embarked:survived\n0.03.01.00.0\n0.01.02.00.0\n0.03.01.01.0\n1.03.00.01.0\n0.03.02.00.0\n1.01.00.00.0\n0.03.00.01.0\n0.03.00.00.0\n1.01.00.01.0\n0.03.00.00.0\n…………\n1.02.00.01.0\n1.03.02.01.0\n0.02.00.00.0\n1.03.02.01.0\n1.01.00.01.0\n0.02.00.00.0\n1.01.00.00.0\n1.02.02.01.0\n0.03.00.00.0\n1.01.02.01.0\n1.01.02.01.0\n\n\n\n\n:metamorph/mode :fit#uuid \"b28d77b7-6436-4f55-a92f-b680cd993199\" {:model-data {:majority-class 1.0, :distinct-labels (0.0 1.0)}, :options {:model-type :metamorph.ml/dummy-classifier}, :id #uuid \"b0f988d2-e657-4e7f-8a57-04c795844115\", :feature-columns [:sex :pclass :embarked], :target-columns [:survived], :target-categorical-maps {:survived #tech.v3.dataset.categorical.CategoricalMap{:lookup-table {0 0, 1 1}, :src-column :survived, :result-datatype :float64}}, :scicloj.metamorph.ml/unsupervised? nil}}\n\n\n(keys ctx-after-train)\n\n\n(:metamorph/data\n :metamorph/mode\n #uuid \"b28d77b7-6436-4f55-a92f-b680cd993199\")\n\n\n\n(vals ctx-after-train)\n\n(:fit\n{:model-data {:majority-class 1.0, :distinct-labels (0.0 1.0)},\n :options {:model-type :metamorph.ml/dummy-classifier},\n :id #uuid \"b0f988d2-e657-4e7f-8a57-04c795844115\",\n :feature-columns [:sex :pclass :embarked],\n :target-columns [:survived],\n :target-categorical-maps\n {:survived\n  {:lookup-table {0 0, 1 1},\n   :src-column :survived,\n   :result-datatype :float64}},\n :scicloj.metamorph.ml/unsupervised? nil}\n)\n\n\n\n(def ctx-after-predict\n  (my-pipeline (assoc ctx-after-train\n                      :metamorph/mode :transform\n                      :metamorph/data test-ds)))\n\n\nctx-after-predict\n\n\n{\n\n\n\n\n\n\n\n\n:metamorph/data\n\n\n_unnamed [178 1]:\n:survived\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n…\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n\n\n:metamorph/mode :transform\n\n\n\n\n\n\n\n\n#uuid \"b28d77b7-6436-4f55-a92f-b680cd993199\"\n\n\n\n{\n\n\n:feature-columns [:sex :pclass :embarked]\n\n\n:target-categorical-maps {:survived #tech.v3.dataset.categorical.CategoricalMap{:lookup-table {0 0, 1 1}, :src-column :survived, :result-datatype :float64}}\n\n\n:target-columns [:survived]\n\n\n:scicloj.metamorph.ml/unsupervised? nil\n\n\n\n\n\n\n\n\n\n:scicloj.metamorph.ml/feature-ds\n\n\nGroup: 0 [178 3]:\n:sex:pclass:embarked\n0.01.02.0\n0.03.00.0\n0.03.00.0\n0.02.00.0\n1.03.01.0\n0.01.00.0\n0.03.00.0\n0.02.00.0\n0.03.00.0\n0.02.00.0\n………\n0.03.00.0\n1.03.00.0\n0.01.02.0\n0.01.02.0\n1.01.00.0\n1.02.00.0\n0.03.00.0\n0.03.00.0\n0.01.02.0\n0.03.00.0\n1.02.00.0\n\n\n\n\n\n:model-data {:majority-class 1.0, :distinct-labels (0.0 1.0)}\n\n\n:id #uuid \"b0f988d2-e657-4e7f-8a57-04c795844115\"\n\n\n\n\n\n\n\n\n\n:scicloj.metamorph.ml/target-ds\n\n\nGroup: 0 [178 1]:\n:survived\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n…\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n:options {:model-type :metamorph.ml/dummy-classifier}\n\n\n}\n\n\n\n\n\n}\n\n\n\n\n(-&gt; ctx-after-predict :metamorph/data :survived)\n\n\n#tech.v3.dataset.column&lt;float64&gt;[178]\n:survived\n[1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000...]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AutoML using metamorph pipelines</span>"
    ]
  },
  {
    "objectID": "automl.html#use-metamorph-pipelines-to-do-model-training-with-higher-level-api",
    "href": "automl.html#use-metamorph-pipelines-to-do-model-training-with-higher-level-api",
    "title": "10  AutoML using metamorph pipelines",
    "section": "10.2 Use metamorph pipelines to do model training with higher level API",
    "text": "10.2 Use metamorph pipelines to do model training with higher level API\nAs user of metamorph.ml we do not need to deal with this low-level details of how metamorph works, we have convenience functions which hide this\nThe following code will do the same as train, but return a context object, which contains the trained model, so it will execute the pipeline, and not only create it.\nIt uses a convenience function mm/fit which generates compliant context maps internally and executes the pipeline as well.\nThe ctx acts a collector of everything “learned” during :fit, mainly the trained model, but it could be as well other information learned from the data during :fit and to be applied at :transform .\n\n(def train-ctx\n  (mm/fit titanic\n          (ml/model {:model-type :metamorph.ml/dummy-classifier})))\n\n(The dummy-classifier model does not have a lot of state, so there is little to see)\n\ntrain-ctx\n\n\n{\n\n\n\n\n\n\n\n\n:metamorph/data\n\n\n_unnamed [889 4]:\n:sex:pclass:embarked:survived\n0.03.00.00.0\n1.01.02.01.0\n1.03.00.01.0\n1.01.00.01.0\n0.03.00.00.0\n0.03.01.00.0\n0.01.00.00.0\n0.03.00.00.0\n1.03.00.01.0\n1.02.02.01.0\n…………\n1.02.00.01.0\n0.03.00.00.0\n1.03.00.00.0\n0.02.00.00.0\n0.03.00.00.0\n1.03.01.00.0\n0.02.00.00.0\n1.01.00.01.0\n1.03.00.00.0\n0.01.02.01.0\n0.03.01.00.0\n\n\n\n\n:metamorph/mode :fit#uuid \"4d0fda5a-5f4f-4da9-9c6e-5e6a32f19445\" {:model-data {:majority-class 1, :distinct-labels (0.0 1.0)}, :options {:model-type :metamorph.ml/dummy-classifier}, :id #uuid \"8fad28d4-1dc8-49ab-ade1-94169e523728\", :feature-columns [:sex :pclass :embarked], :target-columns [:survived], :target-categorical-maps {:survived #tech.v3.dataset.categorical.CategoricalMap{:lookup-table {0 0, 1 1}, :src-column :survived, :result-datatype :float64}}, :scicloj.metamorph.ml/unsupervised? nil}}\n\nTo show the power of pipelines, I start with doing the simplest possible pipeline, and expand then on it.\nwe can already chain train and test with usual functions:\n\n(-&gt;&gt;\n (ml/train train-ds {:model-type :metamorph.ml/dummy-classifier})\n (ml/predict test-ds)\n :survived)\n\n\n#tech.v3.dataset.column&lt;float64&gt;[178]\n:survived\n[1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000...]\n\nthe same with pipelines\n\n(def pipeline\n  (mm/pipeline (ml/model {:model-type :metamorph.ml/dummy-classifier})))\n\n\n(-&gt;&gt;\n (mm/fit-pipe train-ds pipeline)\n (mm/transform-pipe test-ds pipeline)\n :metamorph/data :survived)\n\n\n#tech.v3.dataset.column&lt;float64&gt;[178]\n:survived\n[1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000...]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AutoML using metamorph pipelines</span>"
    ]
  },
  {
    "objectID": "automl.html#create-metamorph-compliant-functions",
    "href": "automl.html#create-metamorph-compliant-functions",
    "title": "10  AutoML using metamorph pipelines",
    "section": "10.3 Create metamorph compliant functions",
    "text": "10.3 Create metamorph compliant functions\nAs said before, a metamorph pipeline is composed of metamorph compliant functions / operations, which take as input and output the ctx. There are three ways to create thoss.\nThese following three expressions create the same metamorph compliant function\n\nimplementing a metamorph compliant function directly via anonymous function\n\n\n(def ops (fn [ctx]\n            (assoc ctx :metamorph/data\n                 (tc/drop-columns (:metamorph/data ctx) [:embarked]))))\n\n\nusing mm/lift which does the same as 1)\n\n\n(def ops (mm/lift tc/drop-columns [:embarked]))\n\n\nusing a name-space containing lifted functions\n\n\n(require '[tablecloth.pipeline])\n\n\n(def ops (tablecloth.pipeline/drop-columns [:embarked]))\n\nAll three create the same pipeline op and can be used to make a pipeline\n\n(mm/pipeline ops)\n\n\n#function[scicloj.metamorph.core/pipeline/local-pipeline--49861]\n\nPipeline as data is as well supported\n\n(def op-spec [[ml/model {:model-type :metamorph.ml/dummy-classifier}]])\n\n\n(mm/-&gt;pipeline op-spec)\n\n\n#function[scicloj.metamorph.core/pipeline/local-pipeline--49861]\n\nAll these do not execute anything, they produce functions which can be executed against a context as part of a metamorph pipeline\nThe ’mm/liftfunction transposes any dataset-&gt;dataset functions into a ctx-&gt;ctx function, while using the emetamorh` convention, as required for metamorph pipeline operations\nFor convenience tablecloth contains a ns where all dataset-&gt;dataset functions are lifted into ctx-&gt;ctx operations, so can be added to pipelines directly without using lift.\nSo a metamorph pipeline can encapsulate arbitray transformation of a dataset in the 2 modes. They can be “stateless” (only chaining the dataset, such as drop-columns) or “state-full”, so they store data in the ctx during :fit and can use it in :transform. In the pipeline above, the trained model is stored in this way.\nThis state is not stored globaly, but inside the pipeline so this makes pipeline execution “isolated”.\nSo now we can add more operations to the pipeline, and nothing else changes, for example drop columns.\nAutomatic ML with metamorph.ml The AutoML support in metamorph consists now in the possibility to create an arbitrary number of different pipelines and have them run against arbitray test/train data splits and it automatically chooses the best model evaluated by by a certain metric.\nhelper for later\n\n(defn make-results-ds [evaluation-results]\n  (-&gt;&gt; evaluation-results\n       flatten\n       (map #(hash-map :options (-&gt; % :test-transform :ctx :model :options)\n                       :used-features (-&gt; % :fit-ctx :used-features)\n                       :mean-accuracy (-&gt; % :test-transform :mean)))\n       tc/dataset))\n\n\n(require '[scicloj.metamorph.ml :as ml]\n         '[scicloj.metamorph.ml.loss :as loss]\n         '[scicloj.metamorph.core :as mm])",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AutoML using metamorph pipelines</span>"
    ]
  },
  {
    "objectID": "automl.html#finding-the-best-model-automatically",
    "href": "automl.html#finding-the-best-model-automatically",
    "title": "10  AutoML using metamorph pipelines",
    "section": "10.4 Finding the best model automatically",
    "text": "10.4 Finding the best model automatically\nThe advantage of the pipelines is even more visible, if we want to have configurable pipelines, and do a grid search to find optimal settings.\nthe following will find the best model across: * 5 different model classes * 6 different selections of used features * k-cross validate this with different test / train splits\n\n(defn make-pipe-fn [model-type features]\n  (mm/pipeline\n   ;; store the used features in ctx, so we can retrieve them at the end\n   (fn [ctx]\n     (assoc ctx :used-features features))\n   (mm/lift tc/select-columns (conj features :survived))\n   {:metamorph/id :model} (ml/model {:model-type model-type})))\n\ncreate a 5-K cross validation split of the data\n\n(def titanic-k-fold (tc/split-&gt;seq ml-basic/numeric-titanic-data :kfold {:seed 12345}))\n\nThe list of the model types we want to try:\n\n(def model-types [:metamorph.ml/dummy-classifier\n                  :smile.classification/random-forest\n                  :smile.classification/logistic-regression\n                  :smile.classification/decision-tree\n                  :smile.classification/ada-boost])\n\nThis uses models from smile only, but could be any metamorph.ml compliant model ( library sklearn-clj wraps all python sklearn models, for example)\nThe list of feature combinations to try for each model:\n\n(def feature-combinations\n  [[:sex :pclass :embarked]\n   [:sex]\n   [:pclass :embarked]\n   [:embarked]\n   [:sex :embarked]\n   [:sex :pclass]])\n\ngenerate 30 pipeline functions:\n\n(def pipe-fns\n  (for [model-type model-types\n        feature-combination feature-combinations]\n    (make-pipe-fn model-type feature-combination)))\n\nExceute all pipelines for all splits in the cross-validations and return best model by classification-accuracy\n\n(def evaluation-results\n  (ml/evaluate-pipelines\n   pipe-fns\n   titanic-k-fold\n   loss/classification-accuracy\n   :accuracy))\n\nBy default it returns the best mode only\n\n(make-results-ds evaluation-results)\n\n_unnamed [1 3]:\n\n\n\n\n\n\n\n\n:used-features\n:mean-accuracy\n:options\n\n\n\n\n[:sex :pclass :embarked]\n0.81107726\n{:model-type :smile.classification/ada-boost}\n\n\n\nThe key observation is here, that the metamorph pipelines allow to not only grid-search over the model hyper-parameters, but as well over arbitrary pipeline variations, like which features to include. Both get handled in the same way.\nWwe can get all results as well:\n\n(def evaluation-results-all\n  (ml/evaluate-pipelines\n   pipe-fns\n   titanic-k-fold\n   loss/classification-accuracy\n   :accuracy\n   {:return-best-crossvalidation-only false\n    :return-best-pipeline-only false}))\n\nIn total it creates and evaluates 5 models * 6 model configurations * 5 CV = 150 models\n\n(-&gt;  evaluation-results-all flatten count)\n\n\n150\n\nWe can find the best as well by hand, it’s the first from the list, when sorted by accuracy.\n\n(-&gt; (make-results-ds evaluation-results-all)\n    (tc/unique-by)\n    (tc/order-by [:mean-accuracy] :desc))\n\n_unnamed [30 3]:\n\n\n\n\n\n\n\n\n:used-features\n:mean-accuracy\n:options\n\n\n\n\n[:sex :pclass :embarked]\n0.81107726\n{:model-type :smile.classification/ada-boost}\n\n\n[:sex]\n0.78633276\n{:model-type :smile.classification/ada-boost}\n\n\n[:sex :pclass]\n0.78633276\n{:model-type :smile.classification/logistic-regression}\n\n\n[:sex :embarked]\n0.78633276\n{:model-type :smile.classification/logistic-regression}\n\n\n[:sex]\n0.78633276\n{:model-type :smile.classification/logistic-regression}\n\n\n[:sex :pclass]\n0.78520917\n{:model-type :smile.classification/random-forest}\n\n\n[:sex :pclass]\n0.77734400\n{:model-type :smile.classification/ada-boost}\n\n\n[:sex :pclass :embarked]\n0.77624579\n{:model-type :smile.classification/random-forest}\n\n\n[:sex :pclass :embarked]\n0.77507776\n{:model-type :smile.classification/logistic-regression}\n\n\n[:sex :pclass]\n0.74138894\n{:model-type :smile.classification/decision-tree}\n\n\n…\n…\n…\n\n\n[:embarked]\n0.63780232\n{:model-type :smile.classification/logistic-regression}\n\n\n[:pclass :embarked]\n0.63333968\n{:model-type :smile.classification/random-forest}\n\n\n[:embarked]\n0.63218435\n{:model-type :smile.classification/ada-boost}\n\n\n[:embarked]\n0.61756491\n{:model-type :smile.classification/random-forest}\n\n\n[:embarked]\n0.61305783\n{:model-type :smile.classification/decision-tree}\n\n\n[:sex :pclass]\n0.38243509\n{:model-type :metamorph.ml/dummy-classifier}\n\n\n[:sex :embarked]\n0.38243509\n{:model-type :metamorph.ml/dummy-classifier}\n\n\n[:embarked]\n0.38243509\n{:model-type :metamorph.ml/dummy-classifier}\n\n\n[:pclass :embarked]\n0.38243509\n{:model-type :metamorph.ml/dummy-classifier}\n\n\n[:sex]\n0.38243509\n{:model-type :metamorph.ml/dummy-classifier}\n\n\n[:sex :pclass :embarked]\n0.38243509\n{:model-type :metamorph.ml/dummy-classifier}",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AutoML using metamorph pipelines</span>"
    ]
  },
  {
    "objectID": "automl.html#best-practices-for-data-transformation-steps-in-or",
    "href": "automl.html#best-practices-for-data-transformation-steps-in-or",
    "title": "10  AutoML using metamorph pipelines",
    "section": "10.5 Best practices for data transformation steps in or",
    "text": "10.5 Best practices for data transformation steps in or\noutside pipeline\n\n(require '[scicloj.metamorph.ml.toydata :as data]\n         '[tech.v3.dataset.modelling :as ds-mod]\n         '[tech.v3.dataset.categorical :as ds-cat]\n         '[tech.v3.dataset :as ds])\n\nWe have seen that we have two ways to transform the input data, outside the pipeline and inside the pipeline\nThese are the total steps from raw data to “into the model” for the titainc use case.\nraw data\n\n(def titanic\n  (:train\n   (data/titanic-ds-split)))\n\nfirst transformation, no metamorph pipeline\n\n(def relevant-titanic-data\n  (-&gt; titanic\n      (tc/select-columns (conj ml-basic/categorical-feature-columns :survived))\n      (tc/drop-missing)\n      (ds/categorical-&gt;number [:sex :pclass :embarked] [0 1 2 \"male\" \"female\" \"S\" \"Q\" \"C\"] :float64)\n      (ds/categorical-&gt;number [:survived] [0 1] :float64)\n      (ds-mod/set-inference-target :survived)))\n\n-&gt; transform via pipelines\n\n(defn make-pipe-fn [model-type features]\n  (mm/pipeline\n   ;; store the used features in ctx, so we can retrieve them at the end\n   (fn [ctx]\n     (assoc ctx :used-features features))\n   (mm/lift tc/select-columns (conj features :survived))\n   {:metamorph/id :model} (ml/model {:model-type model-type})))\n\nWhile it would be technically possible to move all steps from the “first transformation” into the pipeline, by just using the “lifted” form of the transformations, I would not do so, even though this should give the same result.\nI think it is better to separate the steps which are “fixed”, from the steps which are parameterized, so for which we want to find the best values by “trying out”.\nIn my view there are two reasons for this: 1. Debugging: It is harder to debug a pipeline and see the results of steps. We have one macro helping in this: mm/def-ctx 2. Performance: The pipeline is executed lots of times, for every split / variant of the pipeline. It should be faster to do things only once, before the pipeline\n\nsource: notebooks/automl.clj",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>AutoML using metamorph pipelines</span>"
    ]
  },
  {
    "objectID": "interactions_ols.html",
    "href": "interactions_ols.html",
    "title": "11  Ordinary least squares with interactions",
    "section": "",
    "text": "11.1 Additive model\nFirst we build an additive model, which model equation is \\[sales = b0 + b1 * youtube + b2 * facebook\\]\nWe evaluate it,\nand print the result:\nWe have the following metrics:\n\\(RMSE\\)\n\\(R^2\\)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ordinary least squares with interactions</span>"
    ]
  },
  {
    "objectID": "interactions_ols.html#additive-model",
    "href": "interactions_ols.html#additive-model",
    "title": "11  Ordinary least squares with interactions",
    "section": "",
    "text": "(def additive-pipeline\n  (mm/pipeline\n   {:metamorph/id :model}\n   (ml/model {:model-type :smile.regression/ordinary-least-square})))\n\n\n\n(def evaluations\n  (ml/evaluate-pipelines\n   [additive-pipeline]\n   (tc/split-&gt;seq preprocessed-data :holdout)\n   loss/rmse\n   :loss\n   {:other-metrices [{:name :r2\n                      :metric-fn fmstats/r2-determination}]}))\n\n\n\n(-&gt; evaluations flatten first :fit-ctx :model ml/thaw-model)\n\n\nLinear Model:\n\nResiduals:\n       Min          1Q      Median          3Q         Max\n   -6.3045     -1.0313      0.3623      1.4510      3.5406\n\nCoefficients:\n                  Estimate Std. Error    t value   Pr(&gt;|t|)\nIntercept           3.3567     0.4295     7.8156     0.0000 ***\nyoutube             0.0458     0.0016    28.1968     0.0000 ***\nfacebook            0.1893     0.0096    19.6341     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.9743 on 130 degrees of freedom\nMultiple R-squared: 0.9012,    Adjusted R-squared: 0.8997\nF-statistic: 592.8643 on 3 and 130 DF,  p-value: 4.576e-66\n\n\n\n\n(-&gt; evaluations flatten first :test-transform :metric)\n\n\n2.1066041503541197\n\n\n\n(-&gt; evaluations flatten first :test-transform :other-metrices first :metric)\n\n\n0.8909625619872069",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ordinary least squares with interactions</span>"
    ]
  },
  {
    "objectID": "interactions_ols.html#interaction-effects",
    "href": "interactions_ols.html#interaction-effects",
    "title": "11  Ordinary least squares with interactions",
    "section": "11.2 Interaction effects",
    "text": "11.2 Interaction effects\nNow we add interaction effects to it, resulting in this model equation: \\[sales = b0 + b1 * youtube + b2 * facebook + b3 * (youtube * facebook)\\]\n\n(def pipe-interaction\n  (mm/pipeline\n   (tcpipe/add-column :youtube*facebook (fn [ds] (fun/* (ds :youtube) (ds :facebook))))\n   {:metamorph/id :model}(ml/model {:model-type :smile.regression/ordinary-least-square})))\n\nAgain we evaluate the model,\n\n(def evaluations\n  (ml/evaluate-pipelines\n   [pipe-interaction]\n   (tc/split-&gt;seq preprocessed-data :holdout)\n   loss/rmse\n   :loss\n   {:other-metrices [{:name :r2\n                      :metric-fn fmstats/r2-determination}]}))\n\nand print it and the performance metrices:\n\n(-&gt; evaluations flatten first :fit-ctx :model ml/thaw-model)\n\n\nLinear Model:\n\nResiduals:\n       Min          1Q      Median          3Q         Max\n   -4.6301     -0.4783      0.2453      0.7049      1.7305\n\nCoefficients:\n                  Estimate Std. Error    t value   Pr(&gt;|t|)\nIntercept           7.6825     0.3217    23.8826     0.0000 ***\nyoutube             0.0213     0.0017    12.4260     0.0000 ***\nfacebook            0.0449     0.0098     4.5981     0.0000 ***\nyoutube*facebook     0.0008     0.0000    17.1172     0.0000 ***\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.0676 on 129 degrees of freedom\nMultiple R-squared: 0.9725,    Adjusted R-squared: 0.9719\nF-statistic: 1521.6281 on 4 and 129 DF,  p-value: 1.875e-100\n\nAs the multiplcation of youtube*facebook is as well statistically relevant, it suggests that there is indeed an interaction between these 2 predictor variables youtube and facebook.\n\\(RMSE\\)\n\n(-&gt; evaluations flatten first :test-transform :metric)\n\n\n1.2787188267544456\n\n\\(R^2\\)\n\n(-&gt; evaluations flatten first :test-transform :other-metrices first :metric)\n\n\n0.9562320046480208\n\n\\(RMSE\\) and \\(R^2\\) of the intercation model are sligtly better.\nThese results suggest that the model with the interaction term is better than the model that contains only main effects. So, for this specific data, we should go for the model with the interaction model.\n\nsource: notebooks/interactions_ols.clj",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Ordinary least squares with interactions</span>"
    ]
  },
  {
    "objectID": "curve_fitting.html",
    "href": "curve_fitting.html",
    "title": "12  Curve fitting example",
    "section": "",
    "text": "12.1 Prepare data\n:_unnamed [8 2]:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Curve fitting example</span>"
    ]
  },
  {
    "objectID": "curve_fitting.html#prepare-data",
    "href": "curve_fitting.html#prepare-data",
    "title": "12  Curve fitting example",
    "section": "",
    "text": "(def xydata\n  (-&gt; [;; (day-of-month, hour, minute, percentage)\n       ;; d   h   m   p\n       [24,  0, 57, 85]\n       [24, 10, 54, 78]\n       [24, 18, 53, 65]\n       [24, 23, 16, 58]\n       [25,  3, 13, 55]\n       [25,  6, 30, 46]\n       [25, 10, 51, 36]\n       [25, 12, 15, 33]]\n      tc/dataset\n      (tc/rename-columns [:d :h :m :y])\n      (tc/map-columns :x [:d :h :m] (fn [d h m]\n                                      (+ (* d 24 60)\n                                         (* h 60)\n                                         m)))\n      (tc/select-columns [:x :y])))\n\n\nxydata\n\n\n\n\n\n:x\n:y\n\n\n\n\n34617\n85\n\n\n35214\n78\n\n\n35693\n65\n\n\n35956\n58\n\n\n36193\n55\n\n\n36390\n46\n\n\n36651\n36\n\n\n36735\n33\n\n\n\n\n(-&gt; xydata\n    (hanami/plot ht/point-chart\n                 {:XSCALE {:zero false}\n                  :MSIZE 200}))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Curve fitting example</span>"
    ]
  },
  {
    "objectID": "curve_fitting.html#auxiliary-functions",
    "href": "curve_fitting.html#auxiliary-functions",
    "title": "12  Curve fitting example",
    "section": "12.2 Auxiliary functions",
    "text": "12.2 Auxiliary functions\n\n(defn quadroots\n  \"Solve quadratic equation ax^2 + bx + c = 0\n  Return the positive root.\n  We expect it to be &gt;= xdata[0] but do not test for this.\n  Raise exception if there is no positive root.\n  The smaller positive root must be smaller than\n  the first value of `:x` in `xydata`.\"\n  [a b c]\n  (let [disc (- (* b b)\n                (* 4 a c))\n        throw-fn (fn [msg]\n                   (throw (ex-info msg\n                                   {:a a\n                                    :b b\n                                    :c c\n                                    :disc disc})))\n        _ (when-not (pos? disc)\n            (throw-fn  \"Discriminant is not positive\"))\n        sqrt-disc (math/sqrt disc)\n        pre-x1 (+ (- b) sqrt-disc)\n        pre-x2 (- (- b) sqrt-disc)\n        [x1 x2] (if (&lt; pre-x1 pre-x2)\n                  [pre-x2 pre-x1]\n                  [pre-x1 pre-x2])]\n    (when (neg? x1)\n      (throw-fn \"There is no positive root\"))\n    (when (&gt; x2 ((xydata :x) 0))\n      (throw-fn \"The smaller positive root is too big\"))\n    x1))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Curve fitting example</span>"
    ]
  },
  {
    "objectID": "curve_fitting.html#compute",
    "href": "curve_fitting.html#compute",
    "title": "12  Curve fitting example",
    "section": "12.3 Compute",
    "text": "12.3 Compute\n\n(def processed-data\n  (-&gt; xydata\n      (tc/add-column :xx #(fun/sq (:x %)))\n      (modelling/set-inference-target :y)))\n\n\n(def model\n  (-&gt; processed-data\n      (ml/train {:model-type :smile.regression/ordinary-least-square})))\n\nPrinted model summary\n\n(ml/thaw-model model)\n\n\nLinear Model:\n\nResiduals:\n       Min          1Q      Median          3Q         Max\n   -1.5222     -0.4425     -0.2019      1.1882      2.5998\n\nCoefficients:\n                  Estimate Std. Error    t value   Pr(&gt;|t|)\nIntercept       -7831.7120  1677.6175    -4.6684     0.0055 **\nx                   0.4674     0.0940     4.9729     0.0042 **\nxx                 -0.0000     0.0000    -5.2396     0.0034 **\n---------------------------------------------------------------------\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.6179 on 5 degrees of freedom\nMultiple R-squared: 0.9946,    Adjusted R-squared: 0.9925\nF-statistic: 462.0310 on 3 and 5 DF,  p-value: 2.125e-06\n\nCoefficients as data:\n\n(ml/explain model)\n\n\n{:bias -7831.711993948818,\n :coefficients ([:x 0.46735517687137845] [:xx -6.894148026198877E-6])}\n\nPredictions\n\n(-&gt; processed-data\n    (ml/predict model))\n\n:_unnamed [8 1]:\n\n\n\n:y\n\n\n\n\n85.21164462\n\n\n76.81181047\n\n\n66.51897358\n\n\n59.52221756\n\n\n52.40019951\n\n\n45.89077396\n\n\n36.44249850\n\n\n33.20188180",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Curve fitting example</span>"
    ]
  },
  {
    "objectID": "curve_fitting.html#plot",
    "href": "curve_fitting.html#plot",
    "title": "12  Curve fitting example",
    "section": "12.4 Plot",
    "text": "12.4 Plot\n\n(-&gt; processed-data\n    (tc/add-column :predicted-y (fn [ds]\n                                  (-&gt; ds\n                                      (ml/predict model)\n                                      :y)))\n    (hanami/layers {:TITLE \"fitted curve\"}\n                   [(hanami/plot nil ht/point-chart {:MSIZE 200\n                                                     :XSCALE {:zero false}})\n                    (hanami/plot nil ht/line-chart {:Y :predicted-y\n                                                    :MSIZE 4\n                                                    :MCOLOR \"brown\"\n                                                    :OPACITY 0.5\n                                                    :YTITLE \"y\"})]))\n\n\n\nsource: notebooks/curve_fitting.clj",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Curve fitting example</span>"
    ]
  }
]